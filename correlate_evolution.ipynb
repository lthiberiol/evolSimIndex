{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.odr              import Model, Data, RealData, ODR\n",
    "from scipy.stats            import linregress\n",
    "from scipy.optimize         import curve_fit\n",
    "from scipy.spatial.distance import squareform\n",
    "from matplotlib             import pyplot as plt\n",
    "from sklearn.linear_model   import HuberRegressor\n",
    "from copy                   import deepcopy\n",
    "from collections            import Counter\n",
    "from scipy.stats            import pearsonr\n",
    "import igraph  as ig\n",
    "import numpy   as np\n",
    "import seaborn as sns\n",
    "import pandas  as pd\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import ete3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good reads:\n",
    "- https://towardsdatascience.com/total-least-squares-in-comparison-with-ols-and-odr-f050ffc1a86a\n",
    "- https://towardsdatascience.com/linear-regression-in-the-wild-335723a687e8\n",
    "- https://en.wikipedia.org/wiki/Deming_regression\n",
    "- https://en.wikipedia.org/wiki/Total_least_squares\n",
    "- https://stackoverflow.com/questions/44638882/estimate-the-standard-deviation-of-fitted-parameters-in-scipy-odr\n",
    "- https://www.astro.rug.nl/software/kapteyn/kmpfittutorial.html#fitting-data-when-both-variables-have-uncertainties\n",
    "- https://stats.stackexchange.com/a/461968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cd:\n",
    "    \"\"\"\n",
    "    Context manager for changing the current working directory\n",
    "    \"\"\"\n",
    "    def __init__(self, newPath):\n",
    "        self.newPath = os.path.expanduser(newPath)\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.savedPath = os.getcwd()\n",
    "        os.chdir(self.newPath)\n",
    "\n",
    "    def __exit__(self, etype, value, traceback):\n",
    "        os.chdir(self.savedPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cles(lessers, greaters):\n",
    "    \"\"\"Common-Language Effect Size\n",
    "    Probability that a random draw from `greater` is in fact greater\n",
    "    than a random draw from `lesser`.\n",
    "    Args:\n",
    "      lesser, greater: Iterables of comparables.\n",
    "      \n",
    "      https://github.com/ajschumacher/cles/blob/master/cles.py\n",
    "    \"\"\"\n",
    "    if len(lessers) == 0 and len(greaters) == 0:\n",
    "        raise ValueError('At least one argument must be non-empty')\n",
    "    # These values are a bit arbitrary, but make some sense.\n",
    "    # (It might be appropriate to warn for these cases.)\n",
    "    if len(lessers) == 0:\n",
    "        return 1\n",
    "    if len(greaters) == 0:\n",
    "        return 0\n",
    "    numerator = 0\n",
    "    lessers, greaters = sorted(lessers), sorted(greaters)\n",
    "    lesser_index = 0\n",
    "    for greater in greaters:\n",
    "        while lesser_index < len(lessers) and lessers[lesser_index] < greater:\n",
    "            lesser_index += 1\n",
    "        numerator += lesser_index  # the count less than the greater\n",
    "    denominator = len(lessers) * len(greaters)\n",
    "    return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(x, slope):\n",
    "    \"\"\"Basic linear regression 'model'\"\"\"\n",
    "    return (slope * x) + 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_weights(x, y, weight_estimation='gm'):\n",
    "    if weight_estimation == 'gm':\n",
    "        slope = np.std(y)/np.std(x)\n",
    "        x_res = abs(x - line(y, \n",
    "                             slope))\n",
    "        y_res = abs(y - line(x, \n",
    "                             slope))\n",
    "\n",
    "    elif weight_estimation == 'huber':\n",
    "        huber_xy  = HuberRegressor(fit_intercept=False).fit(x.reshape(-1, 1), y)\n",
    "        huber_yx  = HuberRegressor(fit_intercept=False).fit(y.reshape(-1, 1), x)\n",
    "\n",
    "        y_res     = abs(y - line(x, \n",
    "                                 huber_xy.coef_))\n",
    "\n",
    "        x_res     = abs(x - line(y, \n",
    "                                 huber_yx.coef_))\n",
    "        \n",
    "    elif weight_estimation == 'ols':\n",
    "        xy_params = curve_fit(line, x, y)\n",
    "        y_res     = abs(y - line(x, \n",
    "                                 xy_params[0]))\n",
    "        \n",
    "        yx_params = curve_fit(line, y, x)\n",
    "        x_res     = abs(x - line(y, \n",
    "                                 yx_params[0]))\n",
    "    else:\n",
    "        raise Exception('weight_estimation must be \"gm\", \"huber\", or \"ols\"')\n",
    "\n",
    "    #\n",
    "    # if residuals are equal do zero it drives the weight to infinity,\n",
    "    #     and it is good practice not weigh things infinitely\n",
    "    x_res[x_res==0] = 1e-10\n",
    "    y_res[y_res==0] = 1e-10\n",
    "    return(1/abs(x_res), \n",
    "           1/abs(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_odr(x, y, x_weights, y_weights):\n",
    "    mod = Model(line)\n",
    "    dat = Data(x, \n",
    "               y, \n",
    "               wd=x_weights, \n",
    "               we=y_weights\n",
    "    )\n",
    "    odr = ODR(dat, \n",
    "              mod,\n",
    "              beta0=[np.std(y)/np.std(x)])\n",
    "    return(odr.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dist_matrix(aln_file=None, iqtree_path='iqtree', num_threads=1):\n",
    "    path     = '/'.join(aln_file.split('/')[:-1])\n",
    "    filename = aln_file.split('/')[-1]\n",
    "    \n",
    "    with cd(path):\n",
    "        \n",
    "        if not os.path.isfile(f'{filename}.mldist'):\n",
    "            subprocess.call([iqtree_path, \n",
    "                             '-s',     filename, \n",
    "                             '-m',     'LG+G', \n",
    "#                              '-te',    'BIONJ',\n",
    "                             '-nt',    'AUTO',\n",
    "                             '-ntmax', str(num_threads),\n",
    "                             '-keep-ident', '-safe', '-quiet'])\n",
    "        \n",
    "        dist_matrix = pd.read_csv(f'{filename}.mldist', \n",
    "                                  delim_whitespace = True, \n",
    "                                  skiprows         = 1, \n",
    "                                  header           = None,\n",
    "                                  index_col        = 0)\n",
    "        dist_matrix.columns = dist_matrix.index\n",
    "    \n",
    "    return(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_matrices_no_genes(matrix1, matrix2):\n",
    "    \n",
    "    shared_genomes = np.intersect1d(matrix1.index, \n",
    "                                    matrix2.index)\n",
    "    \n",
    "    matrix1 = matrix1.reindex(index  =shared_genomes, \n",
    "                              columns=shared_genomes, \n",
    "                              copy   =True)\n",
    "    matrix2 = matrix2.reindex(index  =shared_genomes, \n",
    "                              columns=shared_genomes, \n",
    "                              copy   =True)\n",
    "        \n",
    "    return(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_copies(matrix1, matrix2, taxa1, taxa2, single_copy=False):\n",
    "    \n",
    "    all_taxa_pairs           = pd.DataFrame()\n",
    "    all_taxa_pairs['taxon1'] = [re.sub('\\|\\d$', '', taxon, flags=re.M)\n",
    "                                for taxon in taxa1.taxon]\n",
    "    all_taxa_pairs['taxon2'] = [re.sub('\\|\\d$', '', taxon, flags=re.M)\n",
    "                                for taxon in taxa2.taxon]\n",
    "\n",
    "    triu_indices = np.triu_indices_from(matrix1, k=1)\n",
    "    condensed1   = matrix1.values[triu_indices]\n",
    "    condensed2   = matrix2.values[triu_indices]\n",
    "\n",
    "    model = Model(line)\n",
    "    data  = Data(condensed1, \n",
    "                 condensed2)\n",
    "    odr   = ODR(data, \n",
    "                model,\n",
    "                beta0=[np.std(condensed2) /\\\n",
    "                       np.std(condensed1)]\n",
    "               )\n",
    "\n",
    "    regression = odr.run()\n",
    "\n",
    "    residual_df = pd.DataFrame(columns=['x_taxon1',   'x_genome1', \n",
    "                                        'x_taxon2',   'x_genome2', \n",
    "\n",
    "                                        'y_taxon1',   'y_genome1', \n",
    "                                        'y_taxon2',   'y_genome2', \n",
    "\n",
    "                                        'x_residual', 'y_residual'],\n",
    "                               data   =zip(matrix1.index[triu_indices[0]],\n",
    "                                           taxa1.iloc[   triu_indices[0], 1].values,\n",
    "                                           \n",
    "                                           matrix1.index[triu_indices[1]],\n",
    "                                           taxa1.iloc[   triu_indices[1], 1].values,\n",
    "\n",
    "                                           matrix2.index[triu_indices[0]],\n",
    "                                           taxa2.iloc[   triu_indices[0], 1].values,\n",
    "                                           \n",
    "                                           matrix2.index[triu_indices[1]],\n",
    "                                           taxa2.iloc[   triu_indices[1], 1].values,\n",
    "\n",
    "                                           abs(regression.delta),\n",
    "                                           abs(regression.eps))\n",
    "                              )\n",
    "    residual_df['residual_total'] = residual_df.x_residual + residual_df.y_residual\n",
    "\n",
    "    within_genomes = ((residual_df.x_genome1 == residual_df.x_genome2) | \n",
    "                      (residual_df.y_genome1 == residual_df.y_genome2))\n",
    "\n",
    "    residual_df.drop(index=residual_df.index[within_genomes], inplace=True)\n",
    "    \n",
    "    for genome in taxa1.genome[taxa1.genome.duplicated()].unique():\n",
    "    \n",
    "        matrix1_homologs = taxa1.loc[taxa1.genome==genome, \n",
    "                                     'taxon'].values\n",
    "        matrix2_homologs = taxa2.loc[taxa2.genome==genome, \n",
    "                                     'taxon'].values\n",
    "\n",
    "        homolog_combinations = pd.DataFrame(columns=['homolog1', \n",
    "                                                     'homolog2', \n",
    "                                                     'residual_sum'])\n",
    "        for homolog1, homolog2 in itertools.product(matrix1_homologs,\n",
    "                                                    matrix2_homologs):\n",
    "            tmp_df = residual_df.query('(x_taxon1 == @homolog1 | x_taxon2 == @homolog1) &'\n",
    "                                       '(y_taxon1 == @homolog2 | y_taxon2 == @homolog2)')\n",
    "\n",
    "            if not tmp_df.shape[0]:\n",
    "                continue\n",
    "\n",
    "            homolog1 = re.sub('\\|\\d$', \n",
    "                              '',\n",
    "                              homolog1, \n",
    "                              flags=re.M)\n",
    "            homolog2 = re.sub('\\|\\d$',\n",
    "                              '', \n",
    "                              homolog2, \n",
    "                              flags=re.M)\n",
    "\n",
    "            homolog_combinations = homolog_combinations.append(\n",
    "                pd.Series(data=[homolog1, \n",
    "                                homolog2, \n",
    "                                tmp_df.residual_total.sum()],\n",
    "                          index=['homolog1', \n",
    "                                 'homolog2', \n",
    "                                 'residual_sum']), \n",
    "                ignore_index=True\n",
    "            )\n",
    "\n",
    "        homolog_combinations.sort_values('residual_sum', inplace=True)\n",
    "        best_pairs = set()\n",
    "        \n",
    "        if single_copy:\n",
    "            first_row = homolog_combinations.iloc[0]\n",
    "            best_pairs.add((first_row.homolog1, first_row.homolog2))\n",
    "        else:\n",
    "            while homolog_combinations.shape[0]:\n",
    "                first_row = homolog_combinations.iloc[0]\n",
    "                best_pairs.add((first_row.homolog1, first_row.homolog2))\n",
    "                homolog_combinations = homolog_combinations.query(f'(homolog1 != \"{first_row.homolog1}\") & '\n",
    "                                                                  f'(homolog2 != \"{first_row.homolog2}\")').copy()\n",
    "            \n",
    "        for homolog1, homolog2 in best_pairs:\n",
    "            indices_to_drop = all_taxa_pairs.query(\n",
    "                '(taxon1 == @homolog1 & taxon2 != @homolog2) |'\n",
    "                '(taxon1 != @homolog1 & taxon2 == @homolog2)'\n",
    "            ).index\n",
    "\n",
    "            all_taxa_pairs.drop(index=indices_to_drop, \n",
    "                                inplace=True)\n",
    "\n",
    "            taxa1.drop(index  =indices_to_drop, \n",
    "                       inplace=True)\n",
    "            taxa2.drop(index  =indices_to_drop, \n",
    "                       inplace=True)\n",
    "    \n",
    "    matrix1 = matrix1.reindex(index  =taxa1.taxon, \n",
    "                              columns=taxa1.taxon, \n",
    "                              copy   =True)\n",
    "    matrix2 = matrix2.reindex(index  =taxa2.taxon, \n",
    "                              columns=taxa2.taxon, \n",
    "                              copy   =True)\n",
    "    \n",
    "    return(matrix1, taxa1, matrix2, taxa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_to_single_copy(matrix1, matrix2, taxa1, taxa2):\n",
    "    \n",
    "    all_taxa_pairs           = pd.DataFrame()\n",
    "    all_taxa_pairs['taxon1'] = [re.sub('\\|\\d$', '', taxon, flags=re.M)\n",
    "                                for taxon in taxa1.taxon]\n",
    "    all_taxa_pairs['taxon2'] = [re.sub('\\|\\d$', '', taxon, flags=re.M)\n",
    "                                for taxon in taxa2.taxon]\n",
    "\n",
    "    triu_indices = np.triu_indices_from(matrix1, k=1)\n",
    "    condensed1   = matrix1.values[triu_indices]\n",
    "    condensed2   = matrix2.values[triu_indices]\n",
    "\n",
    "    model = Model(line)\n",
    "    data  = Data(condensed1, \n",
    "                 condensed2)\n",
    "    odr   = ODR(data, \n",
    "                model,\n",
    "                beta0=[np.std(condensed2) /\\\n",
    "                       np.std(condensed1)]\n",
    "               )\n",
    "\n",
    "    regression = odr.run()\n",
    "\n",
    "    residual_df = pd.DataFrame(columns=['x_taxon1',   'x_genome1', \n",
    "                                        'x_taxon2',   'x_genome2', \n",
    "\n",
    "                                        'y_taxon1',   'y_genome1', \n",
    "                                        'y_taxon2',   'y_genome2', \n",
    "\n",
    "                                        'x_residual', 'y_residual'],\n",
    "                               data   =zip(matrix1.index[triu_indices[0]],\n",
    "                                           taxa1.iloc[   triu_indices[0], 1].values,\n",
    "                                           \n",
    "                                           matrix1.index[triu_indices[1]],\n",
    "                                           taxa1.iloc[   triu_indices[1], 1].values,\n",
    "\n",
    "                                           matrix2.index[triu_indices[0]],\n",
    "                                           taxa2.iloc[   triu_indices[0], 1].values,\n",
    "                                           \n",
    "                                           matrix2.index[triu_indices[1]],\n",
    "                                           taxa2.iloc[   triu_indices[1], 1].values,\n",
    "\n",
    "                                           abs(regression.delta),\n",
    "                                           abs(regression.eps))\n",
    "                              )\n",
    "    residual_df['residual_total'] = residual_df.x_residual + residual_df.y_residual\n",
    "\n",
    "    within_genomes = ((residual_df.x_genome1 == residual_df.x_genome2) | \n",
    "                      (residual_df.y_genome1 == residual_df.y_genome2))\n",
    "\n",
    "    residual_df.drop(index=residual_df.index[within_genomes], inplace=True)\n",
    "    \n",
    "    for genome in taxa1.genome[taxa1.genome.duplicated()].unique():\n",
    "    \n",
    "        matrix1_homologs = taxa1.loc[taxa1.genome==genome, \n",
    "                                     'taxon'].values\n",
    "        matrix2_homologs = taxa2.loc[taxa2.genome==genome, \n",
    "                                     'taxon'].values\n",
    "\n",
    "        homolog_combinations = pd.DataFrame(columns=['homolog1', \n",
    "                                                     'homolog2', \n",
    "                                                     'residual_sum'])\n",
    "        for homolog1, homolog2 in itertools.product(matrix1_homologs,\n",
    "                                                    matrix2_homologs):\n",
    "            tmp_df = residual_df.query('(x_taxon1 == @homolog1 | x_taxon2 == @homolog1) &'\n",
    "                                       '(y_taxon1 == @homolog2 | y_taxon2 == @homolog2)')\n",
    "\n",
    "            if not tmp_df.shape[0]:\n",
    "                continue\n",
    "\n",
    "            homolog1 = re.sub('\\|\\d$', \n",
    "                              '',\n",
    "                              homolog1, \n",
    "                              flags=re.M)\n",
    "            homolog2 = re.sub('\\|\\d$',\n",
    "                              '', \n",
    "                              homolog2, \n",
    "                              flags=re.M)\n",
    "\n",
    "            homolog_combinations = homolog_combinations.append(\n",
    "                pd.Series(data=[homolog1, \n",
    "                                homolog2, \n",
    "                                tmp_df.residual_total.sum()],\n",
    "                          index=['homolog1', \n",
    "                                 'homolog2', \n",
    "                                 'residual_sum']), \n",
    "                ignore_index=True\n",
    "            )\n",
    "\n",
    "        homolog_combinations.sort_values('residual_sum', inplace=True)\n",
    "        best_pairs = set()\n",
    "        first_row  = homolog_combinations.iloc[0]\n",
    "        best_pairs.add((first_row.homolog1, first_row.homolog2))\n",
    "            \n",
    "        for homolog1, homolog2 in best_pairs:\n",
    "            indices_to_drop = all_taxa_pairs.query(\n",
    "                '(taxon1 == @homolog1 & taxon2 != @homolog2) |'\n",
    "                '(taxon1 != @homolog1 & taxon2 == @homolog2)'\n",
    "            ).index\n",
    "\n",
    "            all_taxa_pairs.drop(index=indices_to_drop, \n",
    "                                inplace=True)\n",
    "\n",
    "            taxa1.drop(index  =indices_to_drop, \n",
    "                       inplace=True)\n",
    "            taxa2.drop(index  =indices_to_drop, \n",
    "                       inplace=True)\n",
    "    \n",
    "    matrix1 = matrix1.reindex(index  =taxa1.taxon, \n",
    "                              columns=taxa1.taxon, \n",
    "                              copy   =True)\n",
    "    matrix2 = matrix2.reindex(index  =taxa2.taxon, \n",
    "                              columns=taxa2.taxon, \n",
    "                              copy   =True)\n",
    "    \n",
    "    return(matrix1, taxa1, matrix2, taxa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_matrices(matrix1, matrix2,  gene_sep='_', single_copy=False):\n",
    "    \n",
    "    if gene_sep == '_':\n",
    "        regex = re.compile('^(GC[AF]_\\d+(?:\\.\\d)?)[_|](.*)$')\n",
    "    elif gene_sep == '.':\n",
    "        regex = re.compile('^(\\d+?)\\.(.*)$')\n",
    "    \n",
    "    tmp_taxa = []\n",
    "    for index in matrix1.index:\n",
    "        genome, gene = re.search(regex, index).groups()\n",
    "        tmp_taxa.append([index, genome, gene])\n",
    "\n",
    "    taxa1 = pd.DataFrame(columns=['taxon', 'genome', 'gene'],\n",
    "                         data   =tmp_taxa)\n",
    "\n",
    "    tmp_taxa = []\n",
    "    for index in matrix2.index:\n",
    "        genome, gene = re.search(regex, index).groups()\n",
    "        tmp_taxa.append([index, genome, gene])\n",
    "\n",
    "    taxa2 = pd.DataFrame(columns=['taxon', 'genome', 'gene'],\n",
    "                         data=tmp_taxa)\n",
    "\n",
    "    shared_genomes = np.intersect1d(taxa1.genome.unique(), \n",
    "                                    taxa2.genome.unique())\n",
    "\n",
    "    taxa1 = taxa1[taxa1.genome.isin(shared_genomes)]\n",
    "    taxa2 = taxa2[taxa2.genome.isin(shared_genomes)]\n",
    "\n",
    "    if not taxa1.genome.is_unique or not taxa2.genome.is_unique:\n",
    "    \n",
    "        taxa1_frequency = taxa1.genome.value_counts() \n",
    "        taxa2_frequency = taxa2.genome.value_counts() \n",
    "\n",
    "        for genome in shared_genomes:\n",
    "            genome1_count = taxa1_frequency[genome]\n",
    "            genome2_count = taxa2_frequency[genome]\n",
    "\n",
    "            if genome1_count > 1:\n",
    "                #\n",
    "                # one of the matrices must be traversed in the inversed order to make sure an \n",
    "                #     all VS all combination is obtained. That is the reason of the \"iloc[::-1]\"\n",
    "                #     during the querying\n",
    "                tmp_df = taxa2.iloc[::-1].query('genome == @genome').copy()\n",
    "                for _ in range(genome1_count - 1):\n",
    "                    for index, row in tmp_df.iterrows():\n",
    "                        tmp_row = row.copy()\n",
    "                        tmp_row.taxon += f'|{_}'\n",
    "                        taxa2      = taxa2.append(tmp_row, ignore_index=True)\n",
    "\n",
    "                        reference_name = re.sub('\\|\\d+$', '', tmp_row.taxon, flags=re.M)\n",
    "                        matrix2[    tmp_row.taxon] = matrix2[    reference_name]\n",
    "                        matrix2.loc[tmp_row.taxon] = matrix2.loc[reference_name]\n",
    "\n",
    "\n",
    "            if genome2_count > 1:\n",
    "                #\n",
    "                # as we queried the other matrix in the reverse order, we traverse this one regularly\n",
    "                tmp_df = taxa1.query('genome == @genome').copy()\n",
    "                for _ in range(genome2_count - 1):\n",
    "                    for index, row in tmp_df.iterrows():\n",
    "                        tmp_row = row.copy()\n",
    "                        tmp_row.taxon += f'|{_}'\n",
    "                        taxa1 = taxa1.append(tmp_row, ignore_index=True)\n",
    "\n",
    "                        reference_name = re.sub('\\|\\d+$', '', tmp_row.taxon, flags=re.M)\n",
    "                        matrix1[    tmp_row.taxon] = matrix1[    reference_name]\n",
    "                        matrix1.loc[tmp_row.taxon] = matrix1.loc[reference_name]\n",
    "\n",
    "    taxa1.sort_values('genome', inplace=True)\n",
    "    taxa2.sort_values('genome', inplace=True)\n",
    "\n",
    "    taxa1.reset_index(drop=True, inplace=True)\n",
    "    taxa2.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    all_taxa_pairs           = pd.DataFrame()\n",
    "    all_taxa_pairs['taxon1'] = [re.sub('\\|\\d$', '', taxon, flags=re.M)\n",
    "                                for taxon in taxa1.taxon]\n",
    "    all_taxa_pairs['taxon2'] = [re.sub('\\|\\d$', '', taxon, flags=re.M)\n",
    "                                for taxon in taxa2.taxon]\n",
    "    \n",
    "    matrix1 = matrix1.reindex(index  =taxa1.taxon, \n",
    "                              columns=taxa1.taxon, \n",
    "                              copy   =True)\n",
    "    matrix2 = matrix2.reindex(index  =taxa2.taxon, \n",
    "                              columns=taxa2.taxon, \n",
    "                              copy   =True)\n",
    "    \n",
    "    if not taxa1.genome.is_unique or not taxa2.genome.is_unique:\n",
    "        matrix1, taxa1, matrix2, taxa2 = match_copies(matrix1, matrix2, taxa1, taxa2, single_copy=single_copy)\n",
    "    \n",
    "    return(matrix1, taxa1, matrix2, taxa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_from_tree(tree):\n",
    "    leaf_names = []\n",
    "    for count, node in enumerate(tree.traverse()):\n",
    "        if not node.is_leaf():\n",
    "            node.name = 'node_%i' % count\n",
    "        else:\n",
    "            leaf_names.append(node.name)\n",
    "\n",
    "    edges = []\n",
    "    for node in tree.traverse():\n",
    "        if not node.is_leaf():\n",
    "            for child in node.get_children():\n",
    "                edges.append((node.name,\n",
    "                              child.name,\n",
    "                              child.dist))\n",
    "\n",
    "    dag  = ig.Graph.TupleList(edges     =tuple(edges), \n",
    "                              directed  =False,\n",
    "                              edge_attrs=['weight']\n",
    "                             )\n",
    "    \n",
    "    patristic_distances     = np.array(dag.shortest_paths(source=leaf_names, \n",
    "                                                          target=leaf_names, \n",
    "                                                          weights='weight'))\n",
    "                                       \n",
    "    np.fill_diagonal(patristic_distances, 0.0)\n",
    "    \n",
    "    dist_matrix = pd.DataFrame(index  =leaf_names, \n",
    "                               columns=leaf_names, \n",
    "                               data   =patristic_distances\n",
    "                              )\n",
    "    return(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_coevolution(matrix1, matrix2, \n",
    "                       pearson=False, geneIDs=True,\n",
    "                       gene_sep='_', single_copy=False):\n",
    "    if geneIDs:\n",
    "        matrix1, taxa1, matrix2, taxa2 = balance_matrices(matrix1.copy(), matrix2.copy(), gene_sep, single_copy=single_copy)\n",
    "    else:\n",
    "        matrix1, matrix2 = balance_matrices_no_genes(matrix1.copy(), matrix2.copy())\n",
    "\n",
    "    if matrix1.shape[0] < 10:\n",
    "        return([None, None])\n",
    "\n",
    "    condensed1 = squareform(matrix1.values, checks=False)\n",
    "    condensed2 = squareform(matrix2.values, checks=False)\n",
    "    \n",
    "    if pearson:\n",
    "        return(pearsonr(condensed1,\n",
    "                        condensed2))\n",
    "    \n",
    "    odr_weights = estimate_weights(condensed1, condensed2)\n",
    "    \n",
    "    regression = run_odr(condensed1, \n",
    "                         condensed2, \n",
    "                         *odr_weights)\n",
    "    \n",
    "    mean_x = np.mean(condensed1)\n",
    "    mean_y = np.mean(condensed2)\n",
    "\n",
    "    mean_pred_x = regression.xplus.mean()\n",
    "    mean_pred_y = regression.y.mean()\n",
    "\n",
    "    x_SSres = sum(regression.delta**2)\n",
    "    y_SSres = sum(regression.eps  **2)\n",
    "    SSres   = x_SSres + y_SSres\n",
    "\n",
    "    x_SSreg = sum(\n",
    "        (regression.xplus - mean_pred_x)**2\n",
    "    )\n",
    "    y_SSreg = sum(\n",
    "        (regression.y     - mean_pred_y)**2\n",
    "    )\n",
    "    SSreg   = x_SSreg + y_SSreg\n",
    "\n",
    "    x_SStot = sum(\n",
    "        (condensed1 - mean_x)**2\n",
    "    )\n",
    "    y_SStot = sum(\n",
    "        (condensed2 - mean_y)**2\n",
    "    )\n",
    "    SStot   = x_SStot + y_SStot\n",
    "\n",
    "    r2 = 1 - SSres/SStot\n",
    "#     r2 = SSreg/SStot\n",
    "    \n",
    "    return(regression, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta: [0.49615544]\n",
      "Beta Std Error: [0.00033644]\n",
      "Beta Covariance: [[1.99398167e-06]]\n",
      "Residual Variance: 0.056766828371899364\n",
      "Inverse Condition #: 1.0\n",
      "Reason(s) for Halting:\n",
      "  Sum of squares convergence\n",
      "\n",
      "R**2 = 0.8947962483462916\n"
     ]
    }
   ],
   "source": [
    "# dist1 = run_dist_matrix('/work/clusterEvo/distance_matrices/000284/000284')\n",
    "# dist2 = run_dist_matrix('/work/clusterEvo/distance_matrices/000302/000302')\n",
    "\n",
    "# regression, r2 = assess_coevolution(dist1, dist2)\n",
    "\n",
    "# regression.pprint()\n",
    "# print(f'\\nR**2 = {r2}')\n",
    "\n",
    "## Beta: [0.49615544]\n",
    "## Beta Std Error: [0.00033644]\n",
    "## Beta Covariance: [[1.99398167e-06]]\n",
    "## Residual Variance: 0.056766828371899364\n",
    "## Inverse Condition #: 1.0\n",
    "## Reason(s) for Halting:\n",
    "##   Sum of squares convergence\n",
    "##\n",
    "## R**2 = 0.8947962483462916"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
